{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyenchant in /home/nitish/.local/lib/python3.8/site-packages (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyenchant \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   #importing libraries\n",
    "import re\n",
    "import os\n",
    "import bisect\n",
    "import os,glob\n",
    "import pandas as pd  \n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nitish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nitish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nitish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentsName=os.listdir(\"/home/nitish/Downloads/stories\")   #titles of the files\n",
    "all_files = glob.glob(os.path.join(\"/home/nitish/Downloads/stories\", \"*\"))     # location of the files.....os.path.join  makes concatenation OS independent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_dups(s):   #removing unnecessary letters from extended words\n",
    "    return re.sub(r'(?i)(.)\\1+', r'\\1', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_eng = enchant.Dict(\"en_US\")   # using en_US to see valid english words\n",
    "def Preprocess(text):       \n",
    "    text = re.sub('[^A-Za-z ]+', ' ',text)   # keeping only alphabetical words\n",
    "    words = word_tokenize(text)      # tokenize\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):    #removal stopword\n",
    "                if re.findall(\"([a-zA-Z])\\\\1{3,}\",word):   #checking elongated words.....more than 2 repeated letters\n",
    "                    word = remove_consecutive_dups(word)\n",
    "                if (check_eng.check(word)):                    #is valid english word\n",
    "                    word = word.lower()\n",
    "                    tokens.append(word)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-3e2ac2e2721d>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(all_files[j], names=column, sep=\"\\t,\\n, \")     #load file in a dataframe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documentID={}         # for DocID, keys will be docNumber\n",
    "invertedIndex = {}    #dictionary for inverted index, keys will be tokens\n",
    "\n",
    "for j in range(len(all_files)):       #will open all files one by one\n",
    "    documentText = \"\"\n",
    "    column=['text']                     #column name in dataframe will be \"text\"\n",
    "    df = pd.read_csv(all_files[j], names=column, sep=\"\\t,\\n, \")     #load file in a dataframe\n",
    "    for i in df['text']:\n",
    "        documentText = documentText+i+\" \"\n",
    "    \n",
    "    tokens = set(preprocess(documentText))    # remove duplicate\n",
    "    for token in tokens:                      \n",
    "        if token not in invertedIndex:\n",
    "            posting = []             #if token not already in invertedIndex creating a new entry\n",
    "            posting.append(j+1)\n",
    "            invertedIndex[token] = posting\n",
    "        else:\n",
    "            posting = invertedIndex[token]    #updating docID\n",
    "            bisect.insort(posting, j+1)\n",
    "            invertedIndex[token] = posting\n",
    "    documentID[j+1]=documentsName[j]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47592\n"
     ]
    }
   ],
   "source": [
    "print(len(invertedIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in invertedIndex.keys():            #putting number od docs as first entry in posting of every token\n",
    "    invertedIndex[key].insert(0,len(invertedIndex[key]))    \n",
    "#print(invertedIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(opearand1,opearnd2):\n",
    "    result = []\n",
    "    i = 1\n",
    "    j =1\n",
    "    comparisons = 0\n",
    "    postingList1= invertedIndex[opearand1]\n",
    "    postingList2= invertedIndex[opearand2]\n",
    "    while(i<=postingList1[0] and j<=postingList2[0]):\n",
    "        comparisons= comparisons+1\n",
    "        if(postingList1[i] == postingList2[j]):\n",
    "            result.append(postingList1[i])\n",
    "            i=i+1\n",
    "            j=j+1\n",
    "        elif(postingList1[i] < postingList2[j]):\n",
    "            i=i+1\n",
    "        else:\n",
    "            j=j+1\n",
    "    while(i<=postingList1[0]):\n",
    "        result.append(postingList1[i])  \n",
    "        i = i+1\n",
    "    while(j<=postingList2[0]):\n",
    "        result.append(postingList2[j])  \n",
    "        j = j+1\n",
    "        \n",
    "    result.insert(0,len(result))       \n",
    "  \n",
    "    return comparisons,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(postingList1,postingList2):\n",
    "    result = []\n",
    "    i = 1\n",
    "    j =1\n",
    "    comparisons = 0\n",
    "    while(i<=postingList1[0] and j<=postingList2[0]):\n",
    "        comparisons= comparisons+1\n",
    "        if(postingList1[i] == postingList2[j]):\n",
    "            result.append(postingList1[i])\n",
    "            i=i+1\n",
    "            j=j+1\n",
    "        elif(postingList1[i] < postingList2[j]):\n",
    "            result.append(postingList1[i])\n",
    "            i=i+1\n",
    "        else:\n",
    "            result.append(postingList2[j])\n",
    "            j=j+1\n",
    "            \n",
    "            \n",
    "    while(i<=postingList1[0]):\n",
    "        result.append(postingList1[i]) \n",
    "        i=i+1\n",
    "    while(j<=postingList2[0]):\n",
    "        result.append(postingList2[j])  \n",
    "        j=j+1\n",
    "        \n",
    "    result.insert(0,len(result))       \n",
    "  \n",
    "    return comparisons,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND_NOT(postingList1,postingList2):\n",
    "    result = []\n",
    "    i = 1\n",
    "    j =1\n",
    "    comparisons = 0\n",
    "    while(i<=postingList1[0] and j<=postingList2[0]):\n",
    "        comparisons= comparisons+1\n",
    "        if(postingList1[i] == postingList2[j]):\n",
    "            i=i+1\n",
    "            j=j+1\n",
    "        elif(postingList1[i] < postingList2[j]):\n",
    "            result.append(postingList1[i])\n",
    "            i=i+1\n",
    "        else:\n",
    "            j=j+1\n",
    "            \n",
    "            \n",
    "    while(i<=postingList1[0]):\n",
    "        result.append(postingList1[i]) \n",
    "        i=i+1\n",
    "        \n",
    "    result.insert(0,len(result))       \n",
    "  \n",
    "    return comparisons,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_NOT(postingList1,postingList2):\n",
    "    # total doc AND_NOT {B AND_NOT A}\n",
    "    TotalDoc= []\n",
    "    comparisons = 0\n",
    "    for i in range(1,468):\n",
    "        TotalDoc.append(i)\n",
    "    TotalDoc.insert(0,len(TotalDoc))       \n",
    "    com,result = AND_NOT(postingList2,postingList1)\n",
    "    comparisons = com\n",
    "    com,result = AND_NOT(TotalDoc,result)\n",
    "    comparisons = comparisons + com \n",
    "    \n",
    "    return comparisons,result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT(postingList2):\n",
    "    TotalDoc= []\n",
    "    comparisons = 0\n",
    "    for i in range(1,468):\n",
    "        TotalDoc.append(i)\n",
    "    TotalDoc.insert(0,len(TotalDoc)) \n",
    "    result = []\n",
    "    i = 1\n",
    "    j =1\n",
    "    while(i<=TotalDoc[0] and j<=postingList2[0]):\n",
    "        comparisons= comparisons+1\n",
    "        if(TotalDoc[i] == postingList2[j]):\n",
    "            i=i+1\n",
    "            j=j+1\n",
    "        elif(TotalDoc[i] < postingList2[j]):\n",
    "            result.append(TotalDoc[i])\n",
    "            i=i+1\n",
    "        else:\n",
    "            j=j+1\n",
    "            \n",
    "            \n",
    "    while(i<=TotalDoc[0]):\n",
    "        result.append(TotalDoc[i]) \n",
    "        i=i+1\n",
    "        \n",
    "    result.insert(0,len(result))       \n",
    "  \n",
    "    return comparisons,result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n",
      " lion stood thoughtfully for a moment\n",
      " [OR,OR,OR]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents matched:  265\n",
      "No. of comparisons required:  663\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " telephone,paved, roads\n",
      " [OR NOT, AND NOT]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents matched:  427\n",
      "No. of comparisons required:  814\n"
     ]
    }
   ],
   "source": [
    "NoOfQueries =int(input())            \n",
    "\n",
    "while NoOfQueries!=0 :\n",
    "    query =input()\n",
    "    operators =input() \n",
    "    operands = Preprocess(query)    #Preprocessing the query\n",
    "    operators = operators.split(',')  # preprocessing operators\n",
    "    \n",
    "    operators[0]=operators[0].replace('[',\"\")\n",
    "    operators[len(operators)-1]=operators[len(operators)-1].replace(']',\"\")\n",
    "    for i in range(len(operators)):\n",
    "        operators[i] = operators[i].strip()\n",
    "    \n",
    "    count = len(operators)\n",
    "    result = []\n",
    "    i = 0\n",
    "    comparisons = 0\n",
    "    while(i<count):\n",
    "        if(i==0):\n",
    "            list1 = invertedIndex[operands[0]]\n",
    "        else:\n",
    "            list1 = result\n",
    "        list2 = invertedIndex[operands[i+1]]\n",
    "        if(operators[i]=='AND'):\n",
    "            com,result = AND(list1,list2) \n",
    "        elif(operators[i]=='OR'):\n",
    "            \n",
    "            com,result = OR(list1,list2) \n",
    "        elif(operators[i]=='AND NOT'):\n",
    "            com,result = AND_NOT(list1,list2)\n",
    "        else:\n",
    "            com,result = OR_NOT(list1,list2)\n",
    "            \n",
    "            \n",
    "        comparisons = comparisons + com\n",
    "        i = i+1\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Number of documents matched: \",result[0])\n",
    "    print(\"No. of comparisons required: \",comparisons)\n",
    "    retrivedDocuments = []\n",
    "    for i in result:\n",
    "        retrivedDocuments.append(documentID[i])\n",
    "    #print(\"list of document names retrieved: \",retrivedDocuments)\n",
    "    NoOfQueries = NoOfQueries-1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
